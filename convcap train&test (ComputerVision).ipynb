{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ac14c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import matplotlib; matplotlib.use('Agg')\n",
    "import os\n",
    "import os.path as osp\n",
    "import argparse\n",
    "\n",
    "from train import train \n",
    "from test import test\n",
    "from test_beam import test_beam "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270cd592",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24cdd0c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreFalseAction(option_strings=['--no-attention'], dest='attention', nargs=0, const=False, default=True, type=None, choices=None, help='Use this for convcap without attention', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch Convolutional Image Captioning Model')\n",
    "\n",
    "parser.add_argument('model_dir', help='output directory to save models & results')\n",
    "\n",
    "parser.add_argument('-g', '--gpu', type=int, default=0,\\\n",
    "                    help='gpu device id')\n",
    "\n",
    "parser.add_argument('--coco_root', type=str, default= './data/coco/',\\\n",
    "                    help='directory containing coco dataset train2014, val2014, & annotations')\n",
    "\n",
    "parser.add_argument('-t', '--is_train', type=int, default=1,\\\n",
    "                    help='use 1 to train model')\n",
    "\n",
    "parser.add_argument('-e', '--epochs', type=int, default=30,\\\n",
    "                    help='number of training epochs')\n",
    "\n",
    "parser.add_argument('-b', '--batchsize', type=int, default=32,\\\n",
    "                    help='number of images per training batch')\n",
    "\n",
    "parser.add_argument('-c', '--ncap_per_img', type=int, default=5,\\\n",
    "                    help='ground-truth captions per image in training batch')\n",
    "\n",
    "parser.add_argument('-n', '--num_layers', type=int, default=3,\\\n",
    "                    help='depth of convcap network')\n",
    "\n",
    "parser.add_argument('-m', '--nthreads', type=int, default=4,\\\n",
    "                    help='pytorch data loader threads')\n",
    "\n",
    "# parser.add_argument('-ft', '--finetune_after', type=int, default=8,\\\n",
    "#                     help='epochs after which vgg16 is fine-tuned')\n",
    "\n",
    "parser.add_argument('-lr', '--learning_rate', type=float, default=5e-5,\\\n",
    "                    help='learning rate for convcap')\n",
    "\n",
    "parser.add_argument('-st', '--lr_step_size', type=int, default=15,\\\n",
    "                    help='epochs to decay learning rate after')\n",
    "\n",
    "parser.add_argument('-sc', '--score_select', type=str, default='CIDEr',\\\n",
    "                    help='metric to pick best model')\n",
    "\n",
    "parser.add_argument('--beam_size', type=int, default=1, \\\n",
    "                    help='beam size to use for test') \n",
    "\n",
    "parser.add_argument('--attention', dest='attention', action='store_true', \\\n",
    "                    help='Use this for convcap with attention (by default set)')\n",
    "\n",
    "parser.add_argument('--no-attention', dest='attention', action='store_false', \\\n",
    "                    help='Use this for convcap without attention')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faf0548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.set_defaults(attention=True)\n",
    "\n",
    "args, _ = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aeed33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.finetune_after = 8\n",
    "args.model_dir = 'output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f87f230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import argparse\n",
    "import numpy as np \n",
    "import json\n",
    "import time\n",
    " \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models                                                                     \n",
    "\n",
    "from coco_loader import coco_loader\n",
    "from convcap import convcap\n",
    "from vggfeats import Vgg16Feats\n",
    "from tqdm import tqdm \n",
    "from test import test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92131ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15d80637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n"
     ]
    }
   ],
   "source": [
    "if (args.is_train == 1):\n",
    "    print('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d858dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading annotation file...\n",
      "Found 113287 images in split: train\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading train data ... 3.481802 secs\n"
     ]
    }
   ],
   "source": [
    "t_start = time.time()\n",
    "train_data = coco_loader(args.coco_root, split='train', ncap_per_img=args.ncap_per_img)\n",
    "print('[DEBUG] Loading train data ... %f secs' % (time.time() - t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb3bb293",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(dataset=train_data, num_workers=0, batch_size=args.batchsize, \\\n",
    "                               shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "238f4f06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vgg16Feats(\n",
       "  (features_nopool): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "  )\n",
       "  (features_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_imgcnn = Vgg16Feats()\n",
    "model_imgcnn.cuda()\n",
    "model_imgcnn.train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1045204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convcap(\n",
       "  (emb_0): Embedding(9221, 512, padding_idx=0)\n",
       "  (emb_1): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (imgproj): Linear(in_features=4096, out_features=512, bias=True)\n",
       "  (resproj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,))\n",
       "    (1): Conv1d(512, 1024, kernel_size=(5,), stride=(1,), padding=(4,))\n",
       "    (2): Conv1d(512, 1024, kernel_size=(5,), stride=(1,), padding=(4,))\n",
       "  )\n",
       "  (attention): ModuleList(\n",
       "    (0): AttentionLayer(\n",
       "      (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (1): AttentionLayer(\n",
       "      (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (2): AttentionLayer(\n",
       "      (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier_0): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (classifier_1): Linear(in_features=256, out_features=9221, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convcap model\n",
    "model_convcap = convcap(train_data.numwords, args.num_layers, is_attention=args.attention)\n",
    "model_convcap.cuda()\n",
    "model_convcap.train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97ce59a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(model_convcap.parameters(), lr=args.learning_rate)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=args.lr_step_size, gamma=.1)\n",
    "img_optimizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3353b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = args.batchsize\n",
    "ncap_per_img = args.ncap_per_img\n",
    "batchsize_cap = batchsize*ncap_per_img\n",
    "max_tokens = train_data.max_tokens\n",
    "nbatches = np.int_(np.floor((len(train_data.ids)*1.)/batchsize)) \n",
    "bestscore = .0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df1e0a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_img_per_cap(imgsfeats, imgsfc7, ncap_per_img):\n",
    "    batchsize, featdim, feat_h, feat_w = imgsfeats.size()\n",
    "    batchsize_cap = batchsize*ncap_per_img\n",
    "    imgsfeats = imgsfeats.unsqueeze(1).expand(batchsize, ncap_per_img, featdim, feat_h, feat_w)\n",
    "    imgsfeats = imgsfeats.contiguous().view(batchsize_cap, featdim, feat_h, feat_w)\n",
    "    \n",
    "    batchsize, featdim = imgsfc7.size()\n",
    "    batchsize_cap = batchsize*ncap_per_img\n",
    "    imgsfc7 = imgsfc7.unsqueeze(1).expand(batchsize, ncap_per_img, featdim)\n",
    "    imgsfc7 = imgsfc7.contiguous().view(batchsize_cap, featdim)\n",
    "    \n",
    "    return imgsfeats, imgsfc7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68b32322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad88204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   for epoch in range(args.epochs):\n",
    "# 코드가 잘 돌아가는지 확인하기 위해 2번만 돌려봤습니다. 전 30(args.epochs)번 돌렸습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13bd78c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\410\\anaconda3\\envs\\convcap\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "  0%|                                                                                         | 0/3540 [00:00<?, ?it/s]D:\\2021\\컴퓨터 비전\\eval\\이미지 캡셔닝\\convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 3540/3540 [43:29<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Training epoch 0 has loss 0.011605\n",
      "Loading annotation file...\n",
      "Found 5000 images in split: val\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading val data ... 4.796083 secs\n",
      "[DEBUG] Running inference on val with 156 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/156 [00:00<?, ?it/s]D:\\2021\\컴퓨터 비전\\eval\\이미지 캡셔닝\\test.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 156/156 [00:50<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.26s)\n",
      "creating index...\n",
      "index created!\n",
      "Using 4992/4992 predictions\n",
      "Loading and preparing results...\n",
      "DONE (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 69851, 'reflen': 61712, 'guess': [69851, 64859, 59870, 54881], 'correct': [390, 0, 0, 0]}\n",
      "ratio: 1.1318868291418018\n",
      "Bleu_1: 0.006\n",
      "Bleu_2: 0.000\n",
      "Bleu_3: 0.000\n",
      "Bleu_4: 0.000\n",
      "computing METEOR score...\n",
      "METEOR: 0.013\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.007\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.001\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['java', '-jar', '-Xmx8G', 'spice-1.0.jar', 'C:\\\\Users\\\\410\\\\anaconda3\\\\envs\\\\convcap\\\\lib\\\\site-packages\\\\pycocoevalcap\\\\spice\\\\tmp\\\\tmpfqip9rjt', '-cache', 'C:\\\\Users\\\\410\\\\anaconda3\\\\envs\\\\convcap\\\\lib\\\\site-packages\\\\pycocoevalcap\\\\spice\\\\cache', '-out', 'C:\\\\Users\\\\410\\\\anaconda3\\\\envs\\\\convcap\\\\lib\\\\site-packages\\\\pycocoevalcap\\\\spice\\\\tmp\\\\tmpr4namkiy', '-subset', '-silent']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15700/1626700387.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;31m#Run on validation and obtain score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'val'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_convcap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_convcap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_imgcnn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_imgcnn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore_select\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\2021\\컴퓨터 비전\\eval\\이미지 캡셔닝\\test.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m(args, split, modelfn, model_convcap, model_imgcnn)\u001b[0m\n\u001b[0;32m    101\u001b[0m       \u001b[0mpred_captions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'image_id'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mimg_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'caption'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0moutcap\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m   \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlanguage_eval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_captions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m   \u001b[0mmodel_imgcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\2021\\컴퓨터 비전\\eval\\이미지 캡셔닝\\evaluate.py\u001b[0m in \u001b[0;36mlanguage_eval\u001b[1;34m(input_data, savedir, split)\u001b[0m\n\u001b[0;32m     43\u001b[0m   \u001b[0mcocoEval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCOCOEvalCap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoco\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcocoRes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m   \u001b[0mcocoEval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'image_id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcocoRes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetImgIds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m   \u001b[0mcocoEval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m   \u001b[1;31m# Create output dictionary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\convcap\\lib\\site-packages\\pycocoevalcap\\eval.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mscorers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'computing %s score...'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscorer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\convcap\\lib\\site-packages\\pycocoevalcap\\spice\\spice.py\u001b[0m in \u001b[0;36mcompute_score\u001b[1;34m(self, gts, res)\u001b[0m\n\u001b[0;32m     73\u001b[0m           \u001b[1;34m'-silent'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         ]\n\u001b[1;32m---> 75\u001b[1;33m         subprocess.check_call(spice_cmd, \n\u001b[0m\u001b[0;32m     76\u001b[0m             cwd=os.path.dirname(os.path.abspath(__file__)))\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\convcap\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcheck_call\u001b[1;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcmd\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpopenargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command '['java', '-jar', '-Xmx8G', 'spice-1.0.jar', 'C:\\\\Users\\\\410\\\\anaconda3\\\\envs\\\\convcap\\\\lib\\\\site-packages\\\\pycocoevalcap\\\\spice\\\\tmp\\\\tmpfqip9rjt', '-cache', 'C:\\\\Users\\\\410\\\\anaconda3\\\\envs\\\\convcap\\\\lib\\\\site-packages\\\\pycocoevalcap\\\\spice\\\\cache', '-out', 'C:\\\\Users\\\\410\\\\anaconda3\\\\envs\\\\convcap\\\\lib\\\\site-packages\\\\pycocoevalcap\\\\spice\\\\tmp\\\\tmpr4namkiy', '-subset', '-silent']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    loss_train = 0.\n",
    "    \n",
    "    if(epoch == args.finetune_after):\n",
    "        img_optimizer = optim.RMSprop(model_imgcnn.parameters(), lr=1e-5)\n",
    "        img_scheduler = lr_scheduler.StepLR(img_optimizer, step_size=args.lr_step_size, gamma=.1)\n",
    "\n",
    "    scheduler.step()    \n",
    "    if(img_optimizer):\n",
    "        img_scheduler.step()\n",
    "\n",
    "    #One epoch of train\n",
    "    for batch_idx, (imgs, captions, wordclass, mask, _) in tqdm(enumerate(train_data_loader), total=nbatches):\n",
    "        imgs = imgs.view(batchsize, 3, 224, 224)\n",
    "        wordclass = wordclass.view(batchsize_cap, max_tokens)\n",
    "        mask = mask.view(batchsize_cap, max_tokens)\n",
    "        \n",
    "        imgs_v = Variable(imgs).cuda()\n",
    "        wordclass_v = Variable(wordclass).cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        if(img_optimizer):\n",
    "            img_optimizer.zero_grad() \n",
    "\n",
    "        imgsfeats, imgsfc7 = model_imgcnn(imgs_v)\n",
    "        imgsfeats, imgsfc7 = repeat_img_per_cap(imgsfeats, imgsfc7, ncap_per_img)\n",
    "        _, _, feat_h, feat_w = imgsfeats.size()\n",
    "\n",
    "        if(args.attention == True):\n",
    "            wordact, attn = model_convcap(imgsfeats, imgsfc7, wordclass_v)\n",
    "            attn = attn.view(batchsize_cap, max_tokens, feat_h, feat_w)\n",
    "        else:\n",
    "            wordact, _ = model_convcap(imgsfeats, imgsfc7, wordclass_v)\n",
    "        \n",
    "        wordact = wordact[:,:,:-1]\n",
    "        wordclass_v = wordclass_v[:,1:]\n",
    "        mask = mask[:,1:].contiguous()\n",
    "        \n",
    "        wordact_t = wordact.permute(0, 2, 1).contiguous().view(batchsize_cap*(max_tokens-1), -1)\n",
    "        wordclass_t = wordclass_v.contiguous().view(batchsize_cap*(max_tokens-1), 1)\n",
    "      \n",
    "        maskids = torch.nonzero(mask.view(-1)).numpy().reshape(-1)\n",
    "\n",
    "        if(args.attention == True):\n",
    "            #Cross-entropy loss and attention loss of Show, Attend and Tell\n",
    "            loss = F.cross_entropy(wordact_t[maskids, ...], \\\n",
    "            wordclass_t[maskids, ...].contiguous().view(maskids.shape[0])) \\\n",
    "            + (torch.sum(torch.pow(1. - torch.sum(attn, 1), 2)))\\\n",
    "            /(batchsize_cap*feat_h*feat_w)\n",
    "        else:\n",
    "            loss = F.cross_entropy(wordact_t[maskids, ...], \\\n",
    "            wordclass_t[maskids, ...].contiguous().view(maskids.shape[0]))\n",
    "\n",
    "    loss_train = loss_train + loss.data\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    if(img_optimizer):\n",
    "        img_optimizer.step()\n",
    "\n",
    "    loss_train = (loss_train*1.)/(batch_idx)\n",
    "    print('[DEBUG] Training epoch %d has loss %f' % (epoch, loss_train))\n",
    "\n",
    "    modelfn = osp.join(args.model_dir, 'model.pth')\n",
    "\n",
    "    if(img_optimizer):\n",
    "        img_optimizer_dict = img_optimizer.state_dict()\n",
    "    else:\n",
    "        img_optimizer_dict = None\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model_convcap.state_dict(),\n",
    "        'img_state_dict': model_imgcnn.state_dict(),\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "        'img_optimizer' : img_optimizer_dict,\n",
    "      }, modelfn)\n",
    "\n",
    "    #Run on validation and obtain score\n",
    "    scores = test(args, 'val', model_convcap=model_convcap, model_imgcnn=model_imgcnn)\n",
    "    score = scores[0][args.score_select]\n",
    "\n",
    "    if(score > bestscore):\n",
    "        bestscore = score\n",
    "        print('[DEBUG] Saving model at epoch %d with %s score of %f'\\\n",
    "              % (epoch, args.score_select, score))\n",
    "        bestmodelfn = osp.join(args.model_dir, 'bestmodel.pth')\n",
    "        os.system('cp %s %s' % (modelfn, bestmodelfn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7331abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodelfn = osp.join(args.model_dir, 'bestmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73dba20",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if (osp.exists(bestmodelfn)):\n",
    "    print('if (osp.exists(bestmodelfn)):')\n",
    "    \n",
    "    if (args.beam_size == 1):\n",
    "        print('if (args.beam_size == 1):')\n",
    "        scores = test(args, 'test', modelfn=bestmodelfn)\n",
    "    else:\n",
    "        print('else:')\n",
    "        scores = test_beam(args, 'test', modelfn=bestmodelfn)\n",
    "        \n",
    "    print('TEST set scores')\n",
    "    for k, v in scores[0].items():\n",
    "        print('%s: %f' % (k, v))\n",
    "else:\n",
    "    print('2 else')\n",
    "    raise Exception('No checkpoint found %s' % bestmodelfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f174bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[0].items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176e00f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
